{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10710035,"sourceType":"datasetVersion","datasetId":6637869},{"sourceId":10847574,"sourceType":"datasetVersion","datasetId":6726443}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import nibabel as nib\nimport glob\nimport os\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data.dataset import Dataset\nfrom tqdm import tqdm\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:33:34.924403Z","iopub.execute_input":"2025-03-27T04:33:34.924728Z","iopub.status.idle":"2025-03-27T04:33:34.940209Z","shell.execute_reply.started":"2025-03-27T04:33:34.924699Z","shell.execute_reply":"2025-03-27T04:33:34.939557Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\ndef load_latents(latent_path):\n    r\"\"\"\n    Simple utility to save latents to speed up ldm training\n    :param latent_path:\n    :return:\n    \"\"\"\n    latent_maps = {}\n    for fname in glob.glob(os.path.join(latent_path, '*.nii')):\n        s = nib.load(open(fname, 'rb'))\n        for k, v in s.items():\n            latent_maps[k] = v[0]\n    return latent_maps\n\n\ndef drop_class_condition(class_condition, class_drop_prob, im):\n    if class_drop_prob > 0:\n        class_drop_mask = torch.zeros((im.shape[0], 1), device=im.device).float().uniform_(0,\n                                                                                           1) > class_drop_prob\n        return class_condition * class_drop_mask\n    else:\n        return class_condition","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:33:36.617841Z","iopub.execute_input":"2025-03-27T04:33:36.618134Z","iopub.status.idle":"2025-03-27T04:33:36.623383Z","shell.execute_reply.started":"2025-03-27T04:33:36.618114Z","shell.execute_reply":"2025-03-27T04:33:36.622624Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\ndef validate_class_config(condition_config):\n    assert 'class_condition_config' in condition_config, \\\n        \"Class conditioning desired but class condition config missing\"\n    assert 'num_classes' in condition_config['class_condition_config'], \\\n        \"num_class missing in class condition config\"\n\n\ndef validate_text_config(condition_config):\n    assert 'text_condition_config' in condition_config, \\\n        \"Text conditioning desired but text condition config missing\"\n    assert 'text_embed_dim' in condition_config['text_condition_config'], \\\n        \"text_embed_dim missing in text condition config\"\n    \n\ndef validate_image_config(condition_config):\n    assert 'image_condition_config' in condition_config, \\\n        \"Image conditioning desired but image condition config missing\"\n    assert 'image_condition_input_channels' in condition_config['image_condition_config'], \\\n        \"image_condition_input_channels missing in image condition config\"\n    assert 'image_condition_output_channels' in condition_config['image_condition_config'], \\\n        \"image_condition_output_channels missing in image condition config\"\n    \n\ndef validate_image_conditional_input(cond_input, x):\n    assert 'image' in cond_input, \\\n        \"Model initialized with image conditioning but cond_input has no image information\"\n    assert cond_input['image'].shape[0] == x.shape[0], \\\n        \"Batch size mismatch of image condition and input\"\n    assert cond_input['image'].shape[2] % x.shape[2] == 0, \\\n        \"Height/Width of image condition must be divisible by latent input\"\n\n\ndef validate_class_conditional_input(cond_input, x, num_classes):\n    assert 'class' in cond_input, \\\n        \"Model initialized with class conditioning but cond_input has no class information\"\n    assert cond_input['class'].shape == (x.shape[0], num_classes), \\\n        \"Shape of class condition input must match (Batch Size, )\"\ndef get_config_value(config, key, default_value):\n    return config[key] if key in config else default_value","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:33:36.880689Z","iopub.execute_input":"2025-03-27T04:33:36.880924Z","iopub.status.idle":"2025-03-27T04:33:36.886776Z","shell.execute_reply.started":"2025-03-27T04:33:36.880904Z","shell.execute_reply":"2025-03-27T04:33:36.885907Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# datatset process","metadata":{}},{"cell_type":"code","source":"class AlzDataset(Dataset):\n    \"\"\"\n    Custom dataset class for Alzheimer NIfTI images.\n    \"\"\"\n    def __init__(self, split, im_path, im_size=176, im_channels=1, img_ext='nii',\n                 use_latents=False, latent_path=None, condition_config=None):\n        self.split = split\n        self.im_size = im_size\n        self.im_channels = im_channels\n        self.img_ext = img_ext\n\n        # Latents and conditions\n        self.latent_maps = None\n        self.use_latents = False\n        self.condition_types = [] if condition_config is None else condition_config['condition_types']\n\n        # Load images and labels\n        self.images, self.labels = self.load_images(im_path)\n\n        # Load latents if specified\n        if use_latents and latent_path is not None:\n            latent_maps = load_latents(latent_path)\n            if len(latent_maps) == len(self.images):\n                self.use_latents = True\n                self.latent_maps = latent_maps\n                print(f\"Found {len(self.latent_maps)} latents.\")\n            else:\n                print(\"Latents not found, falling back to image loading.\")\n\n    # def load_images(self, im_path):\n    #     \"\"\"\n    #     Loads image paths and labels (if applicable) from the specified path.\n    #     \"\"\"\n    #     assert os.path.exists(im_path), f\"Images path {im_path} does not exist.\"\n    #     ims, labels = [], []\n\n    #     for d_name in tqdm(os.listdir(im_path)):\n    #         fnames = glob.glob(os.path.join(im_path, d_name, f\"*.{self.img_ext}\"))\n    #         for fname in fnames:\n    #             ims.append(fname)\n    #             if \"class\" in self.condition_types:\n    #                 labels.append(int(d_name))\n\n    #     print(f\"Found {len(ims)} images for split {self.split}.\")\n    #     return ims, labels\n \n\n    def load_images(self, im_path):\n        \n        assert os.path.exists(im_path), f\"Images path {im_path} does not exist.\"\n        ims, labels = [], []\n        class_mapping = {'CN': 0, 'AD': 1, 'MCI': 2}\n    \n        # Verify directory structure\n        expected_dirs = list(class_mapping.keys())\n        existing_dirs = [d for d in os.listdir(im_path) if d in expected_dirs]\n        if not existing_dirs:\n            raise ValueError(f\"No valid class directories found in {im_path}\")\n    \n        for class_name in tqdm(existing_dirs, desc=\"Processing classes\"):\n            class_path = os.path.join(im_path, class_name)\n            skull_path = os.path.join(class_path, 'skull_stripping')\n            \n            if not os.path.exists(skull_path):\n                print(f\"Warning: No skull_stripping directory found in {class_path}\")\n                continue\n    \n            # Find NIfTI files with case-insensitive match\n            fnames = glob.glob(os.path.join(skull_path, f\"*.nii\")) + \\\n                     glob.glob(os.path.join(skull_path, f\"*.NII\"))\n            \n            if not fnames:\n                print(f\"Warning: No NIfTI files found in {skull_path}\")\n                continue\n    \n            ims.extend(fnames)\n            if \"class\" in self.condition_types:\n                labels.extend([class_mapping[class_name]] * len(fnames))\n    \n        print(f\"Found {len(ims)} images ({len(set(labels))} classes) for {self.split} split\")\n        return ims, labels\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Returns the image tensor and condition inputs for the given index.\n        \"\"\"\n        # Set conditioning info\n        cond_inputs = {}\n        if \"class\" in self.condition_types:\n            cond_inputs[\"class\"] = self.labels[idx]\n    \n        # Load image or latent\n        if self.use_latents:\n            im_tensor = self.latent_maps[idx]\n        else:\n            img_nii_path = self.images[idx]\n            im = nib.load(img_nii_path).get_fdata()\n    \n            # Ensure image is 3D and add channel dimension\n            if len(im.shape) == 3:  # (D, H, W)\n                im = np.expand_dims(im, axis=0)  # Add channel dimension -> (1, D, H, W)\n    \n            if len(im.shape) != 4 or im.shape[0] != 1:\n                raise ValueError(f\"Unexpected image shape: {im.shape} at {img_nii_path}\")\n    \n            # Convert to tensor and resize\n            im_tensor = torch.from_numpy(im).float()  # (1, D, H, W)\n            if im_tensor.shape[-1] != self.im_size or im_tensor.shape[-2] != self.im_size:\n                im_tensor = im_tensor.unsqueeze(0)  # Add batch dimension -> (1, 1, D, H, W)\n                im_tensor = F.interpolate(im_tensor, size=(im_tensor.shape[2], self.im_size, self.im_size), mode=\"trilinear\", align_corners=False)\n                im_tensor = im_tensor.squeeze(0)  # Remove batch dimension -> (1, D, H, W)\n\n    \n            # Normalize to range [-1, 1]\n            im_tensor = (im_tensor - im_tensor.min()) / (im_tensor.max() - im_tensor.min())\n            im_tensor = (2 * im_tensor) - 1\n    \n        # Return image and condition inputs\n        if len(self.condition_types) == 0:\n            return im_tensor\n        return im_tensor, cond_inputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:42:13.078741Z","iopub.execute_input":"2025-03-27T05:42:13.079054Z","iopub.status.idle":"2025-03-27T05:42:13.091002Z","shell.execute_reply.started":"2025-03-27T05:42:13.079032Z","shell.execute_reply":"2025-03-27T05:42:13.090034Z"}},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":"# Scheduler","metadata":{}},{"cell_type":"code","source":"class LinearNoiseScheduler:\n    r\"\"\"\n    Class for the linear noise scheduler that is used in DDPM.\n    \"\"\"\n    \n    def __init__(self, num_timesteps, beta_start, beta_end):\n        self.num_timesteps = num_timesteps\n        self.beta_start = beta_start\n        self.beta_end = beta_end\n        # Mimicking how compvis repo creates schedule\n        self.betas = (\n                torch.linspace(beta_start ** 0.5, beta_end ** 0.5, num_timesteps) ** 2\n        )\n        self.alphas = 1. - self.betas\n        self.alpha_cum_prod = torch.cumprod(self.alphas, dim=0)\n        self.sqrt_alpha_cum_prod = torch.sqrt(self.alpha_cum_prod)\n        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1 - self.alpha_cum_prod)\n    \n    def add_noise(self, original, noise, t):\n        r\"\"\"\n        Forward method for diffusion\n        :param original: Image on which noise is to be applied\n        :param noise: Random Noise Tensor (from normal dist)\n        :param t: timestep of the forward process of shape -> (B,)\n        :return:\n        \"\"\"\n        original_shape = original.shape\n        print(original_shape)\n        batch_size = original_shape[0]\n        \n        sqrt_alpha_cum_prod = self.sqrt_alpha_cum_prod.to(original.device)[t].reshape(batch_size)\n        sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod.to(original.device)[t].reshape(batch_size)\n        \n        # Reshape till (B,) becomes (B,1,1,1) if image is (B,C,H,W)\n        for _ in range(len(original_shape) - 1):\n            sqrt_alpha_cum_prod = sqrt_alpha_cum_prod.unsqueeze(-1)\n        for _ in range(len(original_shape) - 1):\n            sqrt_one_minus_alpha_cum_prod = sqrt_one_minus_alpha_cum_prod.unsqueeze(-1)\n        \n        # Apply and Return Forward process equation\n        return (sqrt_alpha_cum_prod.to(original.device) * original\n                + sqrt_one_minus_alpha_cum_prod.to(original.device) * noise)\n    \n    def sample_prev_timestep(self, xt, noise_pred, t):\n        r\"\"\"\n            Use the noise prediction by model to get\n            xt-1 using xt and the nosie predicted\n        :param xt: current timestep sample\n        :param noise_pred: model noise prediction\n        :param t: current timestep we are at\n        :return:\n        \"\"\"\n        x0 = ((xt - (self.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t] * noise_pred)) /\n              torch.sqrt(self.alpha_cum_prod.to(xt.device)[t]))\n        x0 = torch.clamp(x0, -1., 1.)\n        \n        mean = xt - ((self.betas.to(xt.device)[t]) * noise_pred) / (self.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t])\n        mean = mean / torch.sqrt(self.alphas.to(xt.device)[t])\n        \n        if t == 0:\n            return mean, x0\n        else:\n            variance = (1 - self.alpha_cum_prod.to(xt.device)[t - 1]) / (1.0 - self.alpha_cum_prod.to(xt.device)[t])\n            variance = variance * self.betas.to(xt.device)[t]\n            sigma = variance ** 0.5\n            z = torch.randn(xt.shape).to(xt.device)\n            \n            # OR\n            # variance = self.betas[t]\n            # sigma = variance ** 0.5\n            # z = torch.randn(xt.shape).to(xt.device)\n            return mean + sigma * z, x0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:42:19.334376Z","iopub.execute_input":"2025-03-27T05:42:19.334745Z","iopub.status.idle":"2025-03-27T05:42:19.346049Z","shell.execute_reply.started":"2025-03-27T05:42:19.334717Z","shell.execute_reply":"2025-03-27T05:42:19.344942Z"}},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\ndef get_time_embedding(time_steps, temb_dim):\n    r\"\"\"\n    Convert time steps tensor into an embedding using the\n    sinusoidal time embedding formula\n    :param time_steps: 1D tensor of length batch size\n    :param temb_dim: Dimension of the embedding\n    :return: BxD embedding representation of B time steps\n    \"\"\"\n    assert temb_dim % 2 == 0, \"time embedding dimension must be divisible by 2\"\n    \n    # factor = 10000^(2i/d_model)\n    factor = 10000 ** ((torch.arange(\n        start=0, end=temb_dim // 2, dtype=torch.float32, device=time_steps.device) / (temb_dim // 2))\n    )\n    \n    # pos / factor\n    # timesteps B -> B, 1 -> B, temb_dim\n    t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n    return t_emb\n\n\nclass DownBlock(nn.Module):\n    r\"\"\"\n    Down conv block with attention.\n    Sequence of following block\n    1. Resnet block with time embedding\n    2. Attention block\n    3. Downsample\n    \"\"\"\n    \n    def __init__(self, in_channels, out_channels, t_emb_dim,\n                 down_sample, num_heads, num_layers, attn, norm_channels, cross_attn=False, context_dim=None):\n        super().__init__()\n        self.num_layers = num_layers\n        self.down_sample = down_sample\n        self.attn = attn\n        self.context_dim = context_dim\n        self.cross_attn = cross_attn\n        self.t_emb_dim = t_emb_dim\n        self.resnet_conv_first = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n                    nn.SiLU(),\n                    nn.Conv3d(in_channels if i == 0 else out_channels, out_channels,\n                              kernel_size=3, stride=1, padding=1),\n                )\n                for i in range(num_layers)\n            ]\n        )\n        if self.t_emb_dim is not None:\n            self.t_emb_layers = nn.ModuleList([\n                nn.Sequential(\n                    nn.SiLU(),\n                    nn.Linear(self.t_emb_dim, out_channels)\n                )\n                for _ in range(num_layers)\n            ])\n        self.resnet_conv_second = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, out_channels),\n                    nn.SiLU(),\n                    nn.Conv3d(out_channels, out_channels,\n                              kernel_size=3, stride=1, padding=1),\n                )\n                for _ in range(num_layers)\n            ]\n        )\n        \n        if self.attn:\n            self.attention_norms = nn.ModuleList(\n                [nn.GroupNorm(norm_channels, out_channels)\n                 for _ in range(num_layers)]\n            )\n            \n            self.attentions = nn.ModuleList(\n                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n                 for _ in range(num_layers)]\n            )\n        \n        if self.cross_attn:\n            assert context_dim is not None, \"Context Dimension must be passed for cross attention\"\n            self.cross_attention_norms = nn.ModuleList(\n                [nn.GroupNorm(norm_channels, out_channels)\n                 for _ in range(num_layers)]\n            )\n            self.cross_attentions = nn.ModuleList(\n                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n                 for _ in range(num_layers)]\n            )\n            self.context_proj = nn.ModuleList(\n                [nn.Linear(context_dim, out_channels)\n                 for _ in range(num_layers)]\n            )\n\n        self.residual_input_conv = nn.ModuleList(\n            [\n                nn.Conv3d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n                for i in range(num_layers)\n            ]\n        )\n        self.down_sample_conv = nn.Conv3d(out_channels, out_channels,\n                                          4, 2, 1) if self.down_sample else nn.Identity()\n    \n    def forward(self, x, t_emb=None, context=None):\n        out = x\n        for i in range(self.num_layers):\n            # Resnet block of Unet\n            resnet_input = out\n            out = self.resnet_conv_first[i](out)\n            if self.t_emb_dim is not None:\n                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n            out = self.resnet_conv_second[i](out)\n            out = out + self.residual_input_conv[i](resnet_input)\n            \n            if self.attn:\n                # Attention block of Unet\n                batch_size, channels, h, w, d = out.shape\n                in_attn = out.reshape(batch_size, channels, h * w * d)\n                in_attn = self.attention_norms[i](in_attn)\n                in_attn = in_attn.transpose(1, 2)\n                out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w, d)\n                out = out + out_attn\n            \n            if self.cross_attn:\n                assert context is not None, \"context cannot be None if cross attention layers are used\"\n                batch_size, channels, h, w, d = out.shape\n                in_attn = out.reshape(batch_size, channels, h * w * d)\n                in_attn = self.cross_attention_norms[i](in_attn)\n                in_attn = in_attn.transpose(1, 2)\n                assert context.shape[0] == x.shape[0] and context.shape[-1] == self.context_dim\n                context_proj = self.context_proj[i](context)\n                out_attn, _ = self.cross_attentions[i](in_attn, context_proj, context_proj)\n                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w, d)\n                out = out + out_attn\n            \n        # Downsample\n        out = self.down_sample_conv(out)\n        return out\n\n\nclass MidBlock(nn.Module):\n    r\"\"\"\n    Mid conv block with attention.\n    Sequence of following blocks\n    1. Resnet block with time embedding\n    2. Attention block\n    3. Resnet block with time embedding\n    \"\"\"\n    \n    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads, num_layers, norm_channels, cross_attn=None, context_dim=None):\n        super().__init__()\n        self.num_layers = num_layers\n        self.t_emb_dim = t_emb_dim\n        self.context_dim = context_dim\n        self.cross_attn = cross_attn\n        self.resnet_conv_first = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n                    nn.SiLU(),\n                    nn.Conv3d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n                              padding=1),\n                )\n                for i in range(num_layers + 1)\n            ]\n        )\n        \n        if self.t_emb_dim is not None:\n            self.t_emb_layers = nn.ModuleList([\n                nn.Sequential(\n                    nn.SiLU(),\n                    nn.Linear(t_emb_dim, out_channels)\n                )\n                for _ in range(num_layers + 1)\n            ])\n        self.resnet_conv_second = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, out_channels),\n                    nn.SiLU(),\n                    nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n                )\n                for _ in range(num_layers + 1)\n            ]\n        )\n        \n        self.attention_norms = nn.ModuleList(\n            [nn.GroupNorm(norm_channels, out_channels)\n             for _ in range(num_layers)]\n        )\n        \n        self.attentions = nn.ModuleList(\n            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n             for _ in range(num_layers)]\n        )\n        if self.cross_attn:\n            assert context_dim is not None, \"Context Dimension must be passed for cross attention\"\n            self.cross_attention_norms = nn.ModuleList(\n                [nn.GroupNorm(norm_channels, out_channels)\n                 for _ in range(num_layers)]\n            )\n            self.cross_attentions = nn.ModuleList(\n                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n                 for _ in range(num_layers)]\n            )\n            self.context_proj = nn.ModuleList(\n                [nn.Linear(context_dim, out_channels)\n                 for _ in range(num_layers)]\n            )\n        self.residual_input_conv = nn.ModuleList(\n            [\n                nn.Conv3d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n                for i in range(num_layers + 1)\n            ]\n        )\n    \n    def forward(self, x, t_emb=None, context=None):\n        out = x\n        \n        # First resnet block\n        resnet_input = out\n        out = self.resnet_conv_first[0](out)\n        if self.t_emb_dim is not None:\n            out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n        out = self.resnet_conv_second[0](out)\n        out = out + self.residual_input_conv[0](resnet_input)\n        \n        for i in range(self.num_layers):\n            # Attention Block\n            batch_size, channels, h, w, d = out.shape\n            in_attn = out.reshape(batch_size, channels, h * w * d)\n            in_attn = self.attention_norms[i](in_attn)\n            in_attn = in_attn.transpose(1, 2)\n            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w, d)\n            out = out + out_attn\n            \n            if self.cross_attn:\n                assert context is not None, \"context cannot be None if cross attention layers are used\"\n                batch_size, channels, h, w, d = out.shape\n                in_attn = out.reshape(batch_size, channels, h * w * d)\n                in_attn = self.cross_attention_norms[i](in_attn)\n                in_attn = in_attn.transpose(1, 2)\n                assert context.shape[0] == x.shape[0] and context.shape[-1] == self.context_dim\n                context_proj = self.context_proj[i](context)\n                out_attn, _ = self.cross_attentions[i](in_attn, context_proj, context_proj)\n                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w, d)\n                out = out + out_attn\n                \n            \n            # Resnet Block\n            resnet_input = out\n            out = self.resnet_conv_first[i + 1](out)\n            if self.t_emb_dim is not None:\n                out = out + self.t_emb_layers[i + 1](t_emb)[:, :, None, None]\n            out = self.resnet_conv_second[i + 1](out)\n            out = out + self.residual_input_conv[i + 1](resnet_input)\n        \n        return out\n\n\nclass UpBlock(nn.Module):\n    r\"\"\"\n    Up conv block with attention.\n    Sequence of following blocks\n    1. Upsample\n    1. Concatenate Down block output\n    2. Resnet block with time embedding\n    3. Attention Block\n    \"\"\"\n    \n    def __init__(self, in_channels, out_channels, t_emb_dim,\n                 up_sample, num_heads, num_layers, attn, norm_channels):\n        super().__init__()\n        self.num_layers = num_layers\n        self.up_sample = up_sample\n        self.t_emb_dim = t_emb_dim\n        self.attn = attn\n        self.resnet_conv_first = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n                    nn.SiLU(),\n                    nn.Conv3d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n                              padding=1),\n                )\n                for i in range(num_layers)\n            ]\n        )\n        \n        if self.t_emb_dim is not None:\n            self.t_emb_layers = nn.ModuleList([\n                nn.Sequential(\n                    nn.SiLU(),\n                    nn.Linear(t_emb_dim, out_channels)\n                )\n                for _ in range(num_layers)\n            ])\n        \n        self.resnet_conv_second = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, out_channels),\n                    nn.SiLU(),\n                    nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n                )\n                for _ in range(num_layers)\n            ]\n        )\n        if self.attn:\n            self.attention_norms = nn.ModuleList(\n                [\n                    nn.GroupNorm(norm_channels, out_channels)\n                    for _ in range(num_layers)\n                ]\n            )\n            \n            self.attentions = nn.ModuleList(\n                [\n                    nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n                    for _ in range(num_layers)\n                ]\n            )\n            \n        self.residual_input_conv = nn.ModuleList(\n            [\n                nn.Conv3d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n                for i in range(num_layers)\n            ]\n        )\n        self.up_sample_conv = nn.ConvTranspose3d(in_channels, in_channels,\n                                                 4, 2, 1) \\\n            if self.up_sample else nn.Identity()\n    \n    def forward(self, x, out_down=None, t_emb=None):\n        # Upsample\n        x = self.up_sample_conv(x)\n        \n        # Concat with Downblock output\n        if out_down is not None:\n            x = torch.cat([x, out_down], dim=1)\n        \n        out = x\n        for i in range(self.num_layers):\n            # Resnet Block\n            resnet_input = out\n            out = self.resnet_conv_first[i](out)\n            if self.t_emb_dim is not None:\n                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n            out = self.resnet_conv_second[i](out)\n            out = out + self.residual_input_conv[i](resnet_input)\n            \n            # Self Attention\n            if self.attn:\n                batch_size, channels, h, w, d = out.shape\n                in_attn = out.reshape(batch_size, channels, h * w * d)\n                in_attn = self.attention_norms[i](in_attn)\n                in_attn = in_attn.transpose(1, 2)\n                out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w, d)\n                out = out + out_attn\n        return out\n\n\nclass UpBlockUnet(nn.Module):\n    r\"\"\"\n    Up conv block with attention.\n    Sequence of following blocks\n    1. Upsample\n    1. Concatenate Down block output\n    2. Resnet block with time embedding\n    3. Attention Block\n    \"\"\"\n    \n    def __init__(self, in_channels, out_channels, t_emb_dim, up_sample,\n                 num_heads, num_layers, norm_channels, cross_attn=False, context_dim=None):\n        super().__init__()\n        self.num_layers = num_layers\n        self.up_sample = up_sample\n        self.t_emb_dim = t_emb_dim\n        self.cross_attn = cross_attn\n        self.context_dim = context_dim\n        self.resnet_conv_first = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n                    nn.SiLU(),\n                    nn.Conv3d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n                              padding=1),\n                )\n                for i in range(num_layers)\n            ]\n        )\n        \n        if self.t_emb_dim is not None:\n            self.t_emb_layers = nn.ModuleList([\n                nn.Sequential(\n                    nn.SiLU(),\n                    nn.Linear(t_emb_dim, out_channels)\n                )\n                for _ in range(num_layers)\n            ])\n            \n        self.resnet_conv_second = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, out_channels),\n                    nn.SiLU(),\n                    nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n                )\n                for _ in range(num_layers)\n            ]\n        )\n        \n        self.attention_norms = nn.ModuleList(\n            [\n                nn.GroupNorm(norm_channels, out_channels)\n                for _ in range(num_layers)\n            ]\n        )\n        \n        self.attentions = nn.ModuleList(\n            [\n                nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n                for _ in range(num_layers)\n            ]\n        )\n        \n        if self.cross_attn:\n            assert context_dim is not None, \"Context Dimension must be passed for cross attention\"\n            self.cross_attention_norms = nn.ModuleList(\n                [nn.GroupNorm(norm_channels, out_channels)\n                 for _ in range(num_layers)]\n            )\n            self.cross_attentions = nn.ModuleList(\n                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n                 for _ in range(num_layers)]\n            )\n            self.context_proj = nn.ModuleList(\n                [nn.Linear(context_dim, out_channels)\n                 for _ in range(num_layers)]\n            )\n        self.residual_input_conv = nn.ModuleList(\n            [\n                nn.Conv3d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n                for i in range(num_layers)\n            ]\n        )\n        self.up_sample_conv = nn.ConvTranspose3d(in_channels // 2, in_channels // 2,\n                                                 4, 2, 1) \\\n            if self.up_sample else nn.Identity()\n    \n    def forward(self, x, out_down=None, t_emb=None, context=None):\n        x = self.up_sample_conv(x)\n        if out_down is not None:\n            x = torch.cat([x, out_down], dim=1)\n        \n        out = x\n        for i in range(self.num_layers):\n            # Resnet\n            resnet_input = out\n            out = self.resnet_conv_first[i](out)\n            if self.t_emb_dim is not None:\n                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n            out = self.resnet_conv_second[i](out)\n            out = out + self.residual_input_conv[i](resnet_input)\n            # Self Attention\n            batch_size, channels, h, w, d = out.shape\n            in_attn = out.reshape(batch_size, channels, h * w * d)\n            in_attn = self.attention_norms[i](in_attn)\n            in_attn = in_attn.transpose(1, 2)\n            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w, d)\n            out = out + out_attn\n            # Cross Attention\n            if self.cross_attn:\n                assert context is not None, \"context cannot be None if cross attention layers are used\"\n                batch_size, channels, h, w, d = out.shape\n                in_attn = out.reshape(batch_size, channels, h * w * d)\n                in_attn = self.cross_attention_norms[i](in_attn)\n                in_attn = in_attn.transpose(1, 2)\n                assert len(context.shape) == 3, \\\n                    \"Context shape does not match B,_,CONTEXT_DIM\"\n                assert context.shape[0] == x.shape[0] and context.shape[-1] == self.context_dim,\\\n                    \"Context shape does not match B,_,CONTEXT_DIM\"\n                context_proj = self.context_proj[i](context)\n                out_attn, _ = self.cross_attentions[i](in_attn, context_proj, context_proj)\n                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w, d)\n                out = out + out_attn\n        \n        return out\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:42:19.717717Z","iopub.execute_input":"2025-03-27T05:42:19.718004Z","iopub.status.idle":"2025-03-27T05:42:19.757195Z","shell.execute_reply.started":"2025-03-27T05:42:19.717981Z","shell.execute_reply":"2025-03-27T05:42:19.756491Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"class Unet(nn.Module):\n    r\"\"\"\n    Unet model comprising\n    Down blocks, Midblocks and Uplocks\n    \"\"\"\n    \n    def __init__(self, im_channels, model_config):\n        super().__init__()\n        self.down_channels = model_config['down_channels']\n        self.mid_channels = model_config['mid_channels']\n        self.t_emb_dim = model_config['time_emb_dim']\n        self.down_sample = model_config['down_sample']\n        self.num_down_layers = model_config['num_down_layers']\n        self.num_mid_layers = model_config['num_mid_layers']\n        self.num_up_layers = model_config['num_up_layers']\n        self.attns = model_config['attn_down']\n        self.norm_channels = model_config['norm_channels']\n        self.num_heads = model_config['num_heads']\n        self.conv_out_channels = model_config['conv_out_channels']\n        \n        # Validating Unet Model configurations\n        assert self.mid_channels[0] == self.down_channels[-1]\n        assert self.mid_channels[-1] == self.down_channels[-2]\n        assert len(self.down_sample) == len(self.down_channels) - 1\n        assert len(self.attns) == len(self.down_channels) - 1\n        \n        ######## Class Config #####\n        self.class_cond = False\n        \n        self.condition_config = get_config_value(model_config, 'condition_config', None)\n        if self.condition_config is not None:\n            assert 'condition_types' in self.condition_config, 'Condition Type not provided in model config'\n            condition_types = self.condition_config['condition_types']\n            if 'class' in condition_types:\n                validate_class_config(self.condition_config)\n                self.class_cond = True\n                self.num_classes = self.condition_config['class_condition_config']['num_classes']\n        #class embedding information    \n        if self.class_cond:\n            # Rather than using a special null class we dont add the\n            # class embedding information for unconditional generation\n            self.class_emb = nn.Embedding(self.num_classes,\n                                          self.t_emb_dim)\n\n        self.conv_in = nn.Conv3d(im_channels, self.down_channels[0], kernel_size=3, padding=1)\n        self.cond = self.class_cond\n        \n\n        ###################################\n        \n        #This is the FC layer of the time embedding block which takes sinusoidal i/p\n        # Initial projection from sinusoidal time embedding\n        self.t_proj = nn.Sequential(\n            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n            nn.SiLU(),\n            nn.Linear(self.t_emb_dim, self.t_emb_dim)\n        )\n        \n        self.up_sample = list(reversed(self.down_sample))\n        self.downs = nn.ModuleList([])\n        \n        # Build the Downblocks\n        for i in range(len(self.down_channels) - 1):\n            # Cross Attention and Context Dim only needed if text condition is present\n            self.downs.append(DownBlock(self.down_channels[i], self.down_channels[i + 1], self.t_emb_dim,\n                                        down_sample=self.down_sample[i],\n                                        num_heads=self.num_heads,\n                                        num_layers=self.num_down_layers,\n                                        attn=self.attns[i], norm_channels=self.norm_channels,\n                                        # cross_attn=self.text_cond,\n                                        # context_dim=self.text_embed_dim\n                                        ))\n        \n        self.mids = nn.ModuleList([])\n        # Build the Midblocks\n        for i in range(len(self.mid_channels) - 1):\n            self.mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i + 1], self.t_emb_dim,\n                                      num_heads=self.num_heads,\n                                      num_layers=self.num_mid_layers,\n                                      norm_channels=self.norm_channels,\n                                    #   cross_attn=self.text_cond,\n                                    #   context_dim=self.text_embed_dim\n                                      ))\n                \n        self.ups = nn.ModuleList([])\n        # Build the Upblocks\n        for i in reversed(range(len(self.down_channels) - 1)):\n            self.ups.append(\n                UpBlockUnet(self.down_channels[i] * 2, self.down_channels[i - 1] if i != 0 else self.conv_out_channels,\n                            self.t_emb_dim, up_sample=self.down_sample[i],\n                            num_heads=self.num_heads,\n                            num_layers=self.num_up_layers,\n                            norm_channels=self.norm_channels,\n                            # cross_attn=self.text_cond,\n                            # context_dim=self.text_embed_dim\n                            ))\n        \n        self.norm_out = nn.GroupNorm(self.norm_channels, self.conv_out_channels)\n        self.conv_out = nn.Conv2d(self.conv_out_channels, im_channels, kernel_size=3, padding=1)\n    \n    def forward(self, x, t, cond_input=None):\n        # Shapes assuming downblocks are [C1, C2, C3, C4]\n        # Shapes assuming midblocks are [C4, C4, C3]\n        # Shapes assuming downsamples are [True, True, False]\n        # B x C x H x W\n        out = self.conv_in(x)\n        print(out.shape)\n        # B x C1 x H x W\n        context_hidden_states = None\n        # t_emb -> B x t_emb_dim\n        t_emb = get_time_embedding(torch.as_tensor(t).long(), self.t_emb_dim)\n        print(t_emb.shape)\n        t_emb = self.t_proj(t_emb)\n        print(t_emb.shape)\n        \n        ######## Class Conditioning ########\n        if self.class_cond:\n            validate_class_conditional_input(cond_input, x, self.num_classes)\n            class_embed = einsum(cond_input['class'].float(), self.class_emb.weight, 'b n, n d -> b d')\n            t_emb += class_embed\n            print(t_emb.shape)\n        ####################################\n            \n        down_outs = []\n        \n        for idx, down in enumerate(self.downs):\n            down_outs.append(out)\n            out = down(out, t_emb, context_hidden_states)\n        # down_outs  [B x C1 x H x W, B x C2 x H/2 x W/2, B x C3 x H/4 x W/4]\n        # out B x C4 x H/4 x W/4\n        \n        for mid in self.mids:\n            out = mid(out, t_emb, context_hidden_states)\n        # out B x C3 x H/4 x W/4\n        \n        for up in self.ups:\n            down_out = down_outs.pop()\n            out = up(out, down_out, t_emb, context_hidden_states)\n            # out [B x C2 x H/4 x W/4, B x C1 x H/2 x W/2, B x 16 x H x W]\n\n        print(f\"out: {out.shape}\")\n        out = self.norm_out(out)\n        out = nn.SiLU()(out)\n        out = self.conv_out(out)\n        # out B x C x H x W\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:42:19.829561Z","iopub.execute_input":"2025-03-27T05:42:19.829848Z","iopub.status.idle":"2025-03-27T05:42:19.843651Z","shell.execute_reply.started":"2025-03-27T05:42:19.829826Z","shell.execute_reply":"2025-03-27T05:42:19.842694Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"class VAE(nn.Module):\n    def __init__(self, im_channels, model_config):\n        super().__init__()\n        self.down_channels = model_config['down_channels']\n        self.mid_channels = model_config['mid_channels']\n        self.down_sample = model_config['down_sample']\n        self.num_down_layers = model_config['num_down_layers']\n        self.num_mid_layers = model_config['num_mid_layers']\n        self.num_up_layers = model_config['num_up_layers']\n        \n        # To disable attention in Downblock of Encoder and Upblock of Decoder\n        self.attns = model_config['attn_down']\n        \n        # Latent Dimension\n        self.z_channels = model_config['z_channels']\n        self.norm_channels = model_config['norm_channels']\n        self.num_heads = model_config['num_heads']\n        \n        # Assertion to validate the channel information\n        assert self.mid_channels[0] == self.down_channels[-1]\n        assert self.mid_channels[-1] == self.down_channels[-1]\n        assert len(self.down_sample) == len(self.down_channels) - 1\n        assert len(self.attns) == len(self.down_channels) - 1\n        \n        # Wherever we use downsampling in encoder correspondingly use\n        # upsampling in decoder\n        self.up_sample = list(reversed(self.down_sample))\n        \n        ##################### Encoder ######################\n        self.encoder_conv_in = nn.Conv3d(im_channels, self.down_channels[0], kernel_size=3, padding=1)\n        \n        # Downblock + Midblock\n        self.encoder_layers = nn.ModuleList([])\n        for i in range(len(self.down_channels) - 1):\n            self.encoder_layers.append(DownBlock(self.down_channels[i], self.down_channels[i + 1],\n                                                 t_emb_dim=None, down_sample=self.down_sample[i],\n                                                 num_heads=self.num_heads,\n                                                 num_layers=self.num_down_layers,\n                                                 attn=self.attns[i],\n                                                 norm_channels=self.norm_channels))\n        \n        self.encoder_mids = nn.ModuleList([])\n        for i in range(len(self.mid_channels) - 1):\n            self.encoder_mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i + 1],\n                                              t_emb_dim=None,\n                                              num_heads=self.num_heads,\n                                              num_layers=self.num_mid_layers,\n                                              norm_channels=self.norm_channels))\n        \n        self.encoder_norm_out = nn.GroupNorm(self.norm_channels, self.down_channels[-1])\n        self.encoder_conv_out = nn.Conv3d(self.down_channels[-1], 2 * self.z_channels, kernel_size=3, padding=1)\n        \n        # Latent Dimension is 2*Latent because we are predicting mean & variance\n        self.pre_quant_conv = nn.Conv3d(2 * self.z_channels, 2 * self.z_channels, kernel_size=1)\n        ####################################################\n        \n        \n        ##################### Decoder ######################\n        self.post_quant_conv = nn.Conv3d(self.z_channels, self.z_channels, kernel_size=1)\n        self.decoder_conv_in = nn.Conv3d(self.z_channels, self.mid_channels[-1], kernel_size=3, padding=1)\n        \n        # Midblock + Upblock\n        self.decoder_mids = nn.ModuleList([])\n        for i in reversed(range(1, len(self.mid_channels))):\n            self.decoder_mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i - 1],\n                                              t_emb_dim=None,\n                                              num_heads=self.num_heads,\n                                              num_layers=self.num_mid_layers,\n                                              norm_channels=self.norm_channels))\n        \n        self.decoder_layers = nn.ModuleList([])\n        for i in reversed(range(1, len(self.down_channels))):\n            self.decoder_layers.append(UpBlock(self.down_channels[i], self.down_channels[i - 1],\n                                               t_emb_dim=None, up_sample=self.down_sample[i - 1],\n                                               num_heads=self.num_heads,\n                                               num_layers=self.num_up_layers,\n                                               attn=self.attns[i - 1],\n                                               norm_channels=self.norm_channels))\n        \n        self.decoder_norm_out = nn.GroupNorm(self.norm_channels, self.down_channels[0])\n        self.decoder_conv_out = nn.Conv3d(self.down_channels[0], im_channels, kernel_size=3, padding=1)\n    \n    def encode(self, x):\n        out = self.encoder_conv_in(x)\n        for idx, down in enumerate(self.encoder_layers):\n            out = down(out)\n        for mid in self.encoder_mids:\n            out = mid(out)\n        out = self.encoder_norm_out(out)\n        out = nn.SiLU()(out)\n        out = self.encoder_conv_out(out)\n        out = self.pre_quant_conv(out)\n        mean, logvar = torch.chunk(out, 2, dim=1)\n        std = torch.exp(0.5 * logvar)\n        sample = mean + std * torch.randn(mean.shape).to(device=x.device)\n        return sample, out\n    \n    def decode(self, z):\n        out = z\n        out = self.post_quant_conv(out)\n        out = self.decoder_conv_in(out)\n        for mid in self.decoder_mids:\n            out = mid(out)\n        for idx, up in enumerate(self.decoder_layers):\n            out = up(out)\n\n        out = self.decoder_norm_out(out)\n        out = nn.SiLU()(out)\n        out = self.decoder_conv_out(out)\n        return out\n\n    def forward(self, x):\n        z, encoder_output = self.encode(x)\n        out = self.decode(z)\n        return out, encoder_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:42:19.983315Z","iopub.execute_input":"2025-03-27T05:42:19.983647Z","iopub.status.idle":"2025-03-27T05:42:19.997384Z","shell.execute_reply.started":"2025-03-27T05:42:19.983621Z","shell.execute_reply":"2025-03-27T05:42:19.996556Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"class VQVAE(nn.Module):\n    def __init__(self, im_channels, model_config):\n        super().__init__()\n        self.down_channels = model_config['down_channels']\n        self.mid_channels = model_config['mid_channels']\n        self.down_sample = model_config['down_sample']\n        self.num_down_layers = model_config['num_down_layers']\n        self.num_mid_layers = model_config['num_mid_layers']\n        self.num_up_layers = model_config['num_up_layers']\n        \n        # To disable attention in Downblock of Encoder and Upblock of Decoder\n        self.attns = model_config['attn_down']\n        \n        # Latent Dimension\n        self.z_channels = model_config['z_channels']\n        self.codebook_size = model_config['codebook_size']\n        self.norm_channels = model_config['norm_channels']\n        self.num_heads = model_config['num_heads']\n        \n        # Assertion to validate the channel information\n        assert self.mid_channels[0] == self.down_channels[-1]\n        assert self.mid_channels[-1] == self.down_channels[-1]\n        assert len(self.down_sample) == len(self.down_channels) - 1\n        assert len(self.attns) == len(self.down_channels) - 1\n        \n        # Wherever we use downsampling in encoder correspondingly use\n        # upsampling in decoder\n        self.up_sample = list(reversed(self.down_sample))\n        \n        ##################### Encoder ######################\n        self.encoder_conv_in = nn.Conv3d(im_channels, self.down_channels[0], kernel_size=3, padding=2)\n        \n        # Downblock + Midblock\n        self.encoder_layers = nn.ModuleList([])\n        for i in range(len(self.down_channels) - 1):\n            self.encoder_layers.append(DownBlock(self.down_channels[i], self.down_channels[i + 1],\n                                                 t_emb_dim=None, down_sample=self.down_sample[i],\n                                                 num_heads=self.num_heads,\n                                                 num_layers=self.num_down_layers,\n                                                 attn=self.attns[i],\n                                                 norm_channels=self.norm_channels))\n        \n        self.encoder_mids = nn.ModuleList([])\n        for i in range(len(self.mid_channels) - 1):\n            self.encoder_mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i + 1],\n                                              t_emb_dim=None,\n                                              num_heads=self.num_heads,\n                                              num_layers=self.num_mid_layers,\n                                              norm_channels=self.norm_channels))\n        \n        self.encoder_norm_out = nn.GroupNorm(self.norm_channels, self.down_channels[-1])\n        self.encoder_conv_out = nn.Conv3d(self.down_channels[-1], self.z_channels, kernel_size=3, padding=1)\n        \n        # Pre Quantization Convolution\n        self.pre_quant_conv = nn.Conv3d(self.z_channels, self.z_channels, kernel_size=1)\n        \n        # Codebook\n        self.embedding = nn.Embedding(self.codebook_size, self.z_channels)\n        ####################################################\n        \n        ##################### Decoder ######################\n        \n        # Post Quantization Convolution\n        self.post_quant_conv = nn.Conv3d(self.z_channels, self.z_channels, kernel_size=1)\n        self.decoder_conv_in = nn.Conv3d(self.z_channels, self.mid_channels[-1], kernel_size=3, padding=1)\n        \n        # Midblock + Upblock\n        self.decoder_mids = nn.ModuleList([])\n        for i in reversed(range(1, len(self.mid_channels))):\n            self.decoder_mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i - 1],\n                                              t_emb_dim=None,\n                                              num_heads=self.num_heads,\n                                              num_layers=self.num_mid_layers,\n                                              norm_channels=self.norm_channels))\n        \n        self.decoder_layers = nn.ModuleList([])\n        for i in reversed(range(1, len(self.down_channels))):\n            self.decoder_layers.append(UpBlock(self.down_channels[i], self.down_channels[i - 1],\n                                               t_emb_dim=None, up_sample=self.down_sample[i - 1],\n                                               num_heads=self.num_heads,\n                                               num_layers=self.num_up_layers,\n                                               attn=self.attns[i-1],\n                                               norm_channels=self.norm_channels))\n        \n        self.decoder_norm_out = nn.GroupNorm(self.norm_channels, self.down_channels[0])\n        self.decoder_conv_out = nn.Conv3d(self.down_channels[0], im_channels, kernel_size=3, padding=1)\n    \n    def quantize(self, x):\n        B, C, H, W, D = x.shape\n        \n        # B, C, H, W -> B, H, W, C, D\n        x = x.permute(0, 2, 3, 4, 1)\n        \n        # B, H, W, C -> B, H*W, C\n        x = x.reshape(x.size(0), -1, x.size(-1))\n        \n        # Find nearest embedding/codebook vector\n        # dist between (B, H*W, C) and (B, K, C) -> (B, H*W, K)\n        dist = torch.cdist(x, self.embedding.weight[None, :].repeat((x.size(0), 1, 1)))\n        # (B, H*W)\n        min_encoding_indices = torch.argmin(dist, dim=-1)\n        \n        # Replace encoder output with nearest codebook\n        # quant_out -> B*H*W, C\n        quant_out = torch.index_select(self.embedding.weight, 0, min_encoding_indices.view(-1))\n        \n        # x -> B*H*W, C\n        x = x.reshape((-1, x.size(-1)))\n       \n        commmitment_loss = torch.mean((quant_out.detach() - x) ** 2)\n        codebook_loss = torch.mean((quant_out - x.detach()) ** 2)\n        quantize_losses = {\n            'codebook_loss': codebook_loss,\n            'commitment_loss': commmitment_loss\n        }\n        # Straight through estimation\n        quant_out = x + (quant_out - x).detach()\n        \n\n        # quant_out -> B, C, H, W\n        quant_out = quant_out.reshape((B, H, W, D, C)).permute(0, 4, 1, 2, 3) # Reshape back to 3D\n        min_encoding_indices = min_encoding_indices.view(B, H, W, D)   #or add this after dot reshape((-1, quant_out.size(-2), quant_out.size(-1)))\n        return quant_out, quantize_losses, min_encoding_indices\n\n    def encode(self, x):\n        out = self.encoder_conv_in(x)\n        for idx, down in enumerate(self.encoder_layers):\n            out = down(out)\n        for mid in self.encoder_mids:\n            out = mid(out)\n        out = self.encoder_norm_out(out)\n        out = nn.SiLU()(out)\n        out = self.encoder_conv_out(out)\n        out = self.pre_quant_conv(out)\n        out, quant_losses, _ = self.quantize(out)\n        return out, quant_losses\n    \n    def decode(self, z):\n        out = z\n        out = self.post_quant_conv(out)\n        out = self.decoder_conv_in(out)\n        for mid in self.decoder_mids:\n            out = mid(out)\n        for idx, up in enumerate(self.decoder_layers):\n            out = up(out)\n        \n        out = self.decoder_norm_out(out)\n        out = nn.SiLU()(out)\n        out = self.decoder_conv_out(out)\n        return out\n    \n    def forward(self, x):\n        z, quant_losses = self.encode(x)\n        out = self.decode(z)\n        return out, z, quant_losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:42:20.160204Z","iopub.execute_input":"2025-03-27T05:42:20.160542Z","iopub.status.idle":"2025-03-27T05:42:20.176751Z","shell.execute_reply.started":"2025-03-27T05:42:20.160511Z","shell.execute_reply":"2025-03-27T05:42:20.175919Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"config = {\n    \"dataset_params\": {\n        # \"im_path\": \"/kaggle/input/alz-short/alz/train/images/\",\n        \"im_path\": \"/kaggle/input/upama-alzheimer-data/processed_image-001/processed_image/\",\n        \"im_channels\": 1,\n        \"im_size\": 128,\n        \"name\": \"alz\"\n    },\n    \"diffusion_params\": {\n        \"num_timesteps\": 1000,\n        \"beta_start\": 0.0015,\n        \"beta_end\": 0.0195\n    },\n    \"ldm_params\": {\n        \"down_channels\": [128, 256, 256, 256],\n        \"mid_channels\": [256, 256],\n        \"down_sample\": [False, False, False],\n        \"attn_down\": [True, True, True],\n        \"time_emb_dim\": 256,\n        \"norm_channels\": 32,\n        \"num_heads\": 16,\n        \"conv_out_channels\": 128,\n        \"num_down_layers\": 2,\n        \"num_mid_layers\": 2,\n        \"num_up_layers\": 2\n    },\n    \"autoencoder_params\": {\n        \"z_channels\": 3,\n        \"codebook_size\": 20,\n        \"down_channels\": [32, 64, 128],\n        \"mid_channels\": [128, 128],\n        \"down_sample\": [True, True],\n        \"attn_down\": [False, False],\n        \"norm_channels\": 32,\n        \"num_heads\": 8,# changed\n        \"num_down_layers\": 1,\n        \"num_mid_layers\": 1,\n        \"num_up_layers\": 1\n    },\n    \"train_params\": {\n        \"seed\": 1111,\n        \"task_name\": \"alz\",\n        \"ldm_batch_size\": 2,# changed\n        \"autoencoder_batch_size\": 2,# changed\n        \"disc_start\": 1000,\n        \"disc_weight\": 0.5,\n        \"codebook_weight\": 1,\n        \"commitment_beta\": 0.2,\n        \"perceptual_weight\": 1,\n        \"kl_weight\": 0.000005,\n        \"ldm_epochs\": 100,\n        \"autoencoder_epochs\": 10,\n        \"num_samples\": 25,\n        \"num_grid_rows\": 5,\n        \"ldm_lr\": 0.00001,\n        \"autoencoder_lr\": 0.0001,\n        \"autoencoder_acc_steps\": 1,\n        \"autoencoder_img_save_steps\": 8,\n        \"save_latents\": False,\n        \"vae_latent_dir_name\": \"vae_latents\",\n        \"vqvae_latent_dir_name\": \"vqvae_latents\",\n        \"ldm_ckpt_name\": \"ddpm_ckpt.pth\",\n        \"vqvae_autoencoder_ckpt_name\": \"vqvae_autoencoder_ckpt.pth\",\n        \"vae_autoencoder_ckpt_name\": \"vae_autoencoder_ckpt.pth\",\n        \"vqvae_discriminator_ckpt_name\": \"vqvae_discriminator_ckpt.pth\",\n        \"vae_discriminator_ckpt_name\": \"vae_discriminator_ckpt.pth\"\n    }\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:42:20.318363Z","iopub.execute_input":"2025-03-27T05:42:20.318716Z","iopub.status.idle":"2025-03-27T05:42:20.324896Z","shell.execute_reply.started":"2025-03-27T05:42:20.318688Z","shell.execute_reply":"2025-03-27T05:42:20.324137Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.optim import Adam\n\ndiffusion_config = config[\"diffusion_params\"]\ndataset_config = config[\"dataset_params\"]\ndiffusion_model_config = config[\"ldm_params\"]\nautoencoder_model_config = config[\"autoencoder_params\"]\ntrain_config = config[\"train_params\"]\n\nscheduler = LinearNoiseScheduler(num_timesteps=diffusion_config['num_timesteps'],\n                                 beta_start=diffusion_config['beta_start'],\n                                 beta_end=diffusion_config['beta_end'])\n\nim_dataset_cls = {\n    'alz': AlzDataset,\n}.get(dataset_config['name'])\n\nprint(im_dataset_cls)\nif im_dataset_cls is None:\n    raise ValueError(f\"Dataset '{dataset_config['name']}' is not supported.\")\n\nprint(\"Initializing dataset...\")\nim_dataset = im_dataset_cls(\n    split='train',\n    im_path=dataset_config['im_path'],\n    im_size=dataset_config['im_size'],\n    im_channels=dataset_config['im_channels'],\n)\n\ndata_loader = DataLoader(im_dataset,\n                         batch_size=train_config['ldm_batch_size'],\n                         shuffle=True)\n\n# Instantiate the model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = Unet(im_channels=autoencoder_model_config['z_channels'],\n             model_config=diffusion_model_config).to(device)\nmodel.train()\n\n# Load VAE if latents are missing\nif not im_dataset.use_latents:\n    print('Loading vqvae model as latents not present')\n    vae = VQVAE(im_channels=dataset_config['im_channels'],\n                model_config=autoencoder_model_config).to(device)\n    vae.eval()\n\n    # Load VAE checkpoint if found\n    vae_ckpt_path = os.path.join(train_config['task_name'],\n                                  train_config['vqvae_autoencoder_ckpt_name'])\n    if os.path.exists(vae_ckpt_path):\n        print('Loaded VAE checkpoint')\n        vae.load_state_dict(torch.load(vae_ckpt_path, map_location=device))\n\n# Training setup\nnum_epochs = train_config['ldm_epochs']\noptimizer = Adam(model.parameters(), lr=train_config['ldm_lr'])\ncriterion = torch.nn.MSELoss()\n\n# Freeze VAE if latents are not used\nif not im_dataset.use_latents:\n    for param in vae.parameters():\n        param.requires_grad = False\n\n# Training loop\nfor epoch_idx in range(num_epochs):\n    losses = []\n    for im in tqdm(data_loader):\n        optimizer.zero_grad()\n        im = im.float().to(device)\n        print(im.shape)\n        if not im_dataset.use_latents:\n            with torch.no_grad():\n                im, _ = vae.encode(im)\n\n        # Sample random noise\n        noise = torch.randn_like(im).to(device)\n\n        # Sample timestep\n        t = torch.randint(0, diffusion_config['num_timesteps'], (im.shape[0],)).to(device)\n        print(t.shape)\n        # Add noise to images\n        \n        noisy_im = scheduler.add_noise(im, noise, t)\n        print(noisy_im.shape)\n        noise_pred = model(noisy_im, t)\n\n        loss = criterion(noise_pred, noise)\n        losses.append(loss.item())\n        loss.backward()\n        optimizer.step()\n\n    print('Finished epoch:{} | Loss : {:.4f}'.format(\n        epoch_idx + 1,\n        np.mean(losses)))\n\n    torch.save(model.state_dict(), os.path.join(train_config['task_name'],\n                                                train_config['ldm_ckpt_name']))\n\nprint('Done Training ...')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:42:20.488759Z","iopub.execute_input":"2025-03-27T05:42:20.489052Z","iopub.status.idle":"2025-03-27T05:42:22.049213Z","shell.execute_reply.started":"2025-03-27T05:42:20.489031Z","shell.execute_reply":"2025-03-27T05:42:22.048015Z"}},"outputs":[{"name":"stdout","text":"<class '__main__.AlzDataset'>\nInitializing dataset...\n","output_type":"stream"},{"name":"stderr","text":"Processing classes: 100%|| 3/3 [00:00<00:00, 39.27it/s]","output_type":"stream"},{"name":"stdout","text":"Found 3737 images (0 classes) for train split\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Loading vqvae model as latents not present\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/1869 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"torch.Size([2, 1, 197, 128, 128])\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-71-611b20910a3b>\u001b[0m in \u001b[0;36m<cell line: 65>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mim_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_latents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# Sample random noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-69-49fbc83de817>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_conv_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdown\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    718\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             )\n\u001b[0;32m--> 720\u001b[0;31m         return F.conv3d(\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         )\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 822.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 297.12 MiB is free. Process 17916 has 15.60 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 193.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 822.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 297.12 MiB is free. Process 17916 has 15.60 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 193.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":71},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}